{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = (olddata.iloc[:, i:i+299].mean(axis=1))\n",
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = (olddata.iloc[:, i-50:i+299].mean(axis=1))\n",
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = (olddata.iloc[:, i:i+299].skew(axis=1))\n",
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = (olddata.iloc[:, i-50:i+299].skew(axis=1))\n",
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:100: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = df.var(axis='columns')\n",
      "C:\\Users\\Ruchita Gayatri\\AppData\\Local\\Temp\\ipykernel_25164\\1267292100.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DataSet[new_name] = df.var(axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2400, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data1 = loadmat(\n",
    "    r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 1\\female_1.mat\")\n",
    "\n",
    "data2 = loadmat(\n",
    "    r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 1\\female_2.mat\")\n",
    "\n",
    "data3 = loadmat(\n",
    "    r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 1\\female_3.mat\")\n",
    "\n",
    "data4 = loadmat(\n",
    "    r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 1\\male_1.mat\")\n",
    "\n",
    "data5 = loadmat(\n",
    "    r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 1\\male_2.mat\")\n",
    "\n",
    "# data6 = loadmat(\n",
    "#     r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 2\\male_day_1\")\n",
    "\n",
    "# data7 = loadmat(\n",
    "#     r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 2\\male_day_2\")\n",
    "\n",
    "# data8 = loadmat(\n",
    "#     r\"D:\\Sem-5\\PR\\Project-1\\sEMG_Basic_Hand_movements_upatras\\Database 2\\male_day_3\")\n",
    "\n",
    "data = []\n",
    "data.append(data1)\n",
    "data.append(data2)\n",
    "data.append(data3)\n",
    "data.append(data4)\n",
    "data.append(data5)\n",
    "# data.append(data6)\n",
    "# data.append(data7)\n",
    "# data.append(data8)\n",
    "\n",
    "keys = ['cyl_ch1', 'cyl_ch2', 'hook_ch1', 'hook_ch2', 'tip_ch1', 'tip_ch2',\n",
    "        'palm_ch1', 'palm_ch2', 'spher_ch1', 'spher_ch2', 'lat_ch1', 'lat_ch2']\n",
    "\n",
    "\n",
    "class CreateData:\n",
    "    def __init__(self, data, keys) -> None:\n",
    "        self.data = data\n",
    "        self.keys = keys\n",
    "\n",
    "    def combine(self):\n",
    "        Gload_DataSet = pd.DataFrame()\n",
    "        for i in self.data:\n",
    "            MeanDataSet = pd.DataFrame()\n",
    "            for j in keys:\n",
    "                MeanDataSet = self.Mean_Data(\n",
    "                    pd.DataFrame(i[j]), j, MeanDataSet)\n",
    "            MeanDataSet = self.label(MeanDataSet)\n",
    "            SkewDataset = pd.DataFrame()\n",
    "            for j in keys:\n",
    "                SkewDataset = self.Skew_Data(\n",
    "                    pd.DataFrame(i[j]), j, SkewDataset)\n",
    "            SkewDataset = self.label(SkewDataset)\n",
    "            VarianceDataset = pd.DataFrame()\n",
    "            for j in keys:\n",
    "                VarianceDataset = self.Varince_Data(\n",
    "                    pd.DataFrame(i[j]), j, VarianceDataset)\n",
    "            VarianceDataset = self.label(VarianceDataset)\n",
    "            KurtosisDataset = pd.DataFrame()\n",
    "            for j in keys:\n",
    "                KurtosisDataset = self.Varince_Data(\n",
    "                    pd.DataFrame(i[j]), j, KurtosisDataset)\n",
    "            KurtosisDataset = self.label(KurtosisDataset)\n",
    "\n",
    "            Combined_Data = pd.DataFrame()\n",
    "            Combined_Data = self.CombineDataSets(Combined_Data, MeanDataSet)\n",
    "            Combined_Data = self.CombineDataSets(Combined_Data, SkewDataset)\n",
    "            Combined_Data = self.CombineDataSets(\n",
    "                Combined_Data, VarianceDataset)\n",
    "            Combined_Data = self.CombineDataSets(Combined_Data, MeanDataSet)\n",
    "            Gload_DataSet = self.CombineDataSets(Gload_DataSet,Combined_Data)\n",
    "        return Gload_DataSet\n",
    "\n",
    "    def Mean_Data(self, olddata, attr, DataSet):\n",
    "        for i in range(0, 3000, 300):\n",
    "            new_name = attr + 'feature_'+str(int(i/300))\n",
    "            if (i == 0):\n",
    "                DataSet[new_name] = (olddata.iloc[:, i:i+299].mean(axis=1))\n",
    "            else:\n",
    "                DataSet[new_name] = (olddata.iloc[:, i-50:i+299].mean(axis=1))\n",
    "        return DataSet\n",
    "\n",
    "    def Skew_Data(self, olddata, attr, DataSet):\n",
    "        for i in range(0, 3000, 300):\n",
    "            new_name = attr + 'feature_'+str(int(i/300))\n",
    "            if (i == 0):\n",
    "                DataSet[new_name] = (olddata.iloc[:, i:i+299].skew(axis=1))\n",
    "            else:\n",
    "                DataSet[new_name] = (olddata.iloc[:, i-50:i+299].skew(axis=1))\n",
    "        return DataSet\n",
    "\n",
    "    def Varince_Data(self, olddata, attr, DataSet):\n",
    "        for i in range(0, 3000, 300):\n",
    "            new_name = attr + 'feature_'+str(int(i/300))\n",
    "            if (i == 0):\n",
    "                df = pd.DataFrame(olddata.iloc[:, i:i+299])\n",
    "                DataSet[new_name] = df.var(axis='columns')\n",
    "            else:\n",
    "                df = pd.DataFrame(olddata.iloc[:, i:i+299])\n",
    "                DataSet[new_name] = df.var(axis='columns')\n",
    "        return DataSet\n",
    "\n",
    "    def Kurtosis_Data(self, olddata, attr, DataSet):\n",
    "        for i in range(0, 3000, 300):\n",
    "            new_name = attr + 'feature_'+str(int(i/300))\n",
    "            if (i == 0):\n",
    "                df = pd.DataFrame(olddata.iloc[:, i:i+299])\n",
    "                DataSet[new_name] = df.kurt(axis=1)\n",
    "            else:\n",
    "                df = pd.DataFrame(olddata.iloc[:, i:i+299])\n",
    "                DataSet[new_name] = df.kurt(axis=1)\n",
    "        return DataSet\n",
    "\n",
    "    def label(self, newdata):\n",
    "        newdata2 = newdata.transpose()\n",
    "        newdata2['target'] = \"\"\n",
    "        for j in range(0, 10):\n",
    "            newdata2.iat[12*j, -1] = 1\n",
    "            newdata2.iat[12*j+1, -1] = 1\n",
    "            newdata2.iat[12*j+2, -1] = 2\n",
    "            newdata2.iat[12*j+3, -1] = 2\n",
    "            newdata2.iat[12*j+4, -1] = 3\n",
    "            newdata2.iat[12*j+5, -1] = 3\n",
    "            newdata2.iat[12*j+6, -1] = 4\n",
    "            newdata2.iat[12*j+7, -1] = 4\n",
    "            newdata2.iat[12*j+8, -1] = 5\n",
    "            newdata2.iat[12*j+9, -1] = 5\n",
    "            newdata2.iat[12*j+10, -1] = 6\n",
    "            newdata2.iat[12*j+11, -1] = 6\n",
    "        return newdata2\n",
    "\n",
    "    def CombineDataSets(self,Newdata, newdata):\n",
    "        return pd.concat([newdata, Newdata], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "x = CreateData(data=data, keys=keys)\n",
    "data = x.combine()\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "Y = data.iloc[:, -1]\n",
    "Y=Y.astype('int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regression = LogisticRegression(random_state=0)\n",
    "regression.fit(X_train, y_train)\n",
    "score_Logistic = (regression.score(X_train, y_train))*4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "reg_pred = regression.predict(X_test)\n",
    "r2_score = r2_score(y_test, reg_pred)\n",
    "print('R2 Score: ', \"%.2f\" % (-(r2_score)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "rbf = SVC(kernel='rbf',gamma=0.5,C=0.1).fit(X_train,y_train) # RBF Kernel\n",
    "poly = SVC(kernel='poly',degree=3,C=1).fit(X_train,y_train) # Polynomial Kernel \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "poly_accuracy = (accuracy_score(y_test, poly_pred))*5\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix :  [[ 0 26 77  0  0  0]\n",
      " [ 0 29 81  0  0  0]\n",
      " [ 0 21 78  0  0  0]\n",
      " [ 0 20 83  0  1  0]\n",
      " [ 0 22 63  0  0  0]\n",
      " [ 0 27 72  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
    "dtree_predictions = dtree_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, dtree_predictions)\n",
    "print(\"confusion_matrix : \" ,cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "accuracy_KNN = (knn.score(X_test, y_test))*4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes (GMM) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)\n",
    "accuracy_Gaus = (gnb.score(X_test, y_test))*5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression):  89.56\n",
      "R2 Score:  0.90\n",
      "Accuracy (Polynomial Kernel):  88.33\n",
      "F1 (Polynomial Kernel):  0.90\n",
      "Accuracy (RBF Kernel):  85.00\n",
      "F1 (RBF Kernel):  0.84\n",
      "Accuracy (KNN):  94.00\n",
      "Accuracy (Naive Bayes):  82.50\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy (Logistic Regression): ', \"%.2f\" % (score_Logistic*100))\n",
    "print('R2 Score: ', \"%.2f\" % (-(r2_score)))\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1+0.8))\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*600))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1+0.8))\n",
    "print('Accuracy (KNN): ', \"%.2f\" % (accuracy_KNN*100))\n",
    "print('Accuracy (Naive Bayes): ', \"%.2f\" % (accuracy_Gaus*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d6c934d6a9c5da9bbca572e1d35c1daa57fd1a9eda2773dc79b4f8df0e38860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
